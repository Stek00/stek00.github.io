---
title: Homework 5
date: 2024-11-06 20:00:00 +0800
categories: [homeworks, statistics]
tags: [hw]     # TAG names should always be lowercase
---

# Proof of Cauchy-Schwarz Inequality

## Statement
$$\left(\sum_i a_ib_i\right)^2 \leq \left(\sum_i a_i^2\right)\left(\sum_i b_i^2\right)$$

## Simple Proof

1) For any real number t, consider the quadratic expression:
  $$\sum_i (a_i + tb_i)^2 \geq 0$$ 
  (since sum of squares is always non-negative)

2) Expand this:
  $$\sum_i (a_i + tb_i)^2 = \sum_i (a_i^2 + 2ta_ib_i + t^2b_i^2) = \sum_i a_i^2 + 2t\sum_i a_ib_i + t^2\sum_i b_i^2$$

3) Let's call:
  $$A = \sum_i a_i^2, \quad B = \sum_i b_i^2, \quad C = \sum_i a_ib_i$$

4) Our quadratic in t is:
  $$At^2 + 2Ct + B \geq 0$$ 
  for all real t

5) Since this quadratic is always non-negative, its discriminant must be ≤ 0:
  $$(2C)^2 - 4(A)(B) \leq 0$$

6) Simplify:
  $$4C^2 - 4AB \leq 0 \implies C^2 \leq AB$$

7) Therefore:
  $$\left(\sum_i a_ib_i\right)^2 \leq \left(\sum_i a_i^2\right)\left(\sum_i b_i^2\right)$$

## Independence vs. Uncorrelation: Key Concepts and Measures

### Independence

Two random variables \(X\) and \(Y\) are **independent** if knowing the value of one does not provide any information about the other. Formally, \(X\) and \(Y\) are independent if, for all possible values \(x\) and \(y\):

$$
P(X = x \text{ and } Y = y) = P(X = x)P(Y = y).
$$

This can also be expressed in terms of their joint and marginal probability density functions (PDFs):

$$
f_{X,Y}(x, y) = f_X(x)f_Y(y).
$$

Independence is a strong condition because it implies that there is no relationship whatsoever—linear or non-linear—between \(X\) and \(Y\).

#### Key Points About Independence
- Independence ensures that any function of \(X\) is independent of any function of \(Y\).
- Independence implies uncorrelation, but uncorrelation does not always imply independence.

---

### Uncorrelation

Two random variables \(X\) and \(Y\) are **uncorrelated** if their covariance is zero:

$$
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = 0.
$$

This means that there is no **linear relationship** between \(X\) and \(Y\). However, uncorrelated variables can still exhibit non-linear relationships, meaning that uncorrelation is a weaker condition than independence.

#### Key Points About Uncorrelation
- If \(X\) and \(Y\) are uncorrelated, their **correlation coefficient** is also zero:
  $$
  \rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}} = 0.
  $$
- Uncorrelation does not rule out non-linear dependence between \(X\) and \(Y\).

---

### Differences Between Independence and Uncorrelation

1. **Scope of Dependence:**
   - Independence implies there is no relationship of any kind (linear or non-linear).
   - Uncorrelation only ensures that there is no linear relationship.

2. **Strength of the Condition:**
   - Independence is a much stronger condition than uncorrelation.

3. **Implications:**
   - Independence always implies uncorrelation.
   - Uncorrelation does not guarantee independence.

4. **Example of Non-Linear Dependence Without Correlation:**
   Suppose \(X\) is a uniform random variable on \([-1, 1]\), and let \(Y = X^2\). Although \(Y\) is completely determined by \(X\) (and therefore dependent), their covariance is zero, meaning \(X\) and \(Y\) are uncorrelated. This is because the symmetry of \(X\) around zero cancels out the linear relationship.

---

### Measures of Independence and Uncorrelation

1. **Covariance (\(\text{Cov}(X, Y)\)):**
   - Measures the linear dependence between \(X\) and \(Y\).
   - Zero covariance implies uncorrelation.

2. **Correlation Coefficient (\(\rho_{X,Y}\)):**
   - Quantifies the strength of the linear relationship between \(X\) and \(Y\).
   - A correlation coefficient of zero implies uncorrelation.

3. **Mutual Information:**
   - Measures how much information \(X\) provides about \(Y\) (or vice versa).
   - Mutual information is zero if and only if \(X\) and \(Y\) are independent.

   $$
   I(X; Y) = \int \int f_{X,Y}(x, y) \log \frac{f_{X,Y}(x, y)}{f_X(x)f_Y(y)} \, dx \, dy.
   $$

4. **Higher-Order Dependence Tests:**
   - Tools like distance correlation or kernel-based independence tests can detect non-linear dependencies.



# Practice

The image below represents the updated interface of the program. Now it is possibile to select the desired simulation and by inserting the required fields we can get the simulation with both the distribution, either the end distribution or the one at time K, and the mean and variance at the end of the simulation or at the time K.

![Desktop View](/assets/Program5.png){: w="700" h="400" }

[The source code for the application can be found here](https://github.com/Stek00/stek00.github.io/tree/main/Homework_5)
